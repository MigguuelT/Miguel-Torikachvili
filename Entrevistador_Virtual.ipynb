{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe6n4o94ObZXA2e9dljMzp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MigguuelT/Miguel-Torikachvili/blob/main/Entrevistador_Virtual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T53IIWO63D0i",
        "outputId": "2f17cc2c-2f0f-4fe7-954e-242200ac16b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.15.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSPx8p1py8-N"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import display, Markdown\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import Layout\n",
        "import os  # Importe o módulo os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Configure a chave da API (substitua pela sua chave real)\n",
        "# genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Seleciona o modelo Gemini Pro\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# Define os widgets de entrada\n",
        "vaga_input = widgets.Text(description=\"Vaga Desejada:\", layout=Layout(width='50%'))\n",
        "area_input = widgets.Text(description=\"Área de Atuação:\", layout=Layout(width='50%'))\n",
        "tom_input = widgets.Dropdown(\n",
        "    options=['formal', 'informal', 'profissional e amigável', 'direto e objetivo'],\n",
        "    description=\"Tom da Entrevista:\",\n",
        "    layout=Layout(width='50%')\n",
        ")\n",
        "conhecimentos_input = widgets.Text(description=\"Conhecimentos Específicos:\", layout=Layout(width='50%'))\n",
        "enviar_config_button = widgets.Button(description=\"Configurar Entrevista\")\n",
        "pergunta_output = widgets.Output()\n",
        "resposta_input = widgets.Textarea(description=\"Sua Resposta:\", layout=Layout(width='80%', height='150px'))\n",
        "enviar_resposta_button = widgets.Button(description=\"Enviar Resposta\")\n",
        "feedback_output = widgets.Output()\n",
        "historico_output = widgets.Output()\n",
        "fim_entrevista = False\n",
        "respostas_usuario = []\n",
        "perguntas_realizadas = []\n",
        "\n",
        "system_instruction_base = \"Você é um entrevistador para a vaga de {vaga} na área de {area}. A entrevista deve ter um tom {tom}. Faça perguntas típicas para esta função, incluindo perguntas comportamentais focadas em trabalho em equipe e resolução de problemas, além de algumas perguntas sobre o conhecimento de {conhecimentos} e o interesse do candidato pela empresa. Por favor, analise a minha resposta à cada pergunta. Forneça feedback sobre a clareza da minha resposta, o uso de exemplos e como eu poderia ter respondido de forma mais eficaz, talvez usando a metodologia STAR.\"\n",
        "system_instruction = \"\"\n",
        "historico_conversa = []\n",
        "\n",
        "def format_markdown(md_string):\n",
        "    display(Markdown(md_string))\n",
        "\n",
        "def configurar_entrevista(b):\n",
        "    global system_instruction, fim_entrevista, respostas_usuario, perguntas_realizadas\n",
        "    vaga = vaga_input.value\n",
        "    area = area_input.value\n",
        "    tom = tom_input.value\n",
        "    conhecimentos = conhecimentos_input.value\n",
        "\n",
        "    system_instruction = system_instruction_base.format(vaga=vaga, area=area, tom=tom, conhecimentos=conhecimentos)\n",
        "\n",
        "    fim_entrevista = False\n",
        "    respostas_usuario = []\n",
        "    perguntas_realizadas = []\n",
        "    historico_conversa.clear()\n",
        "    with historico_output:\n",
        "        historico_output.clear_output()\n",
        "    with pergunta_output:\n",
        "        pergunta_output.clear_output()\n",
        "    with feedback_output:\n",
        "        feedback_output.clear_output()\n",
        "\n",
        "    with pergunta_output:\n",
        "        format_markdown(\"**Entrevista configurada. Clique em 'Próxima Pergunta' para começar.**\")\n",
        "\n",
        "def finalizar_entrevista():\n",
        "    global respostas_usuario, perguntas_realizadas, system_instruction\n",
        "    feedback_total_prompt = f\"{system_instruction}\\n\\nRespostas do candidato:\\n\"\n",
        "    for i, resposta in enumerate(respostas_usuario):\n",
        "        feedback_total_prompt += f\"Pergunta {i+1}: {perguntas_realizadas[i]}\\nResposta: {resposta}\\n\\n\"\n",
        "    feedback_total_prompt += \"Forneça um feedback geral sobre o desempenho do candidato na entrevista, destacando os pontos fortes e as áreas de melhoria.\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(feedback_total_prompt)\n",
        "        feedback_total = response.text\n",
        "\n",
        "        with feedback_output:\n",
        "            feedback_output.clear_output()\n",
        "            format_markdown(f\"**Entrevista Encerrada. Feedback Total:**\\n{feedback_total}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        with feedback_output:\n",
        "            feedback_output.clear_output()\n",
        "            format_markdown(f\"**Erro ao gerar feedback total:** {e}\")\n",
        "\n",
        "def enviar_resposta(b):\n",
        "    global system_instruction, fim_entrevista, respostas_usuario, perguntas_realizadas\n",
        "    resposta_usuario = resposta_input.value.lower()\n",
        "    resposta_input.value = \"\"\n",
        "\n",
        "    if resposta_usuario == \"fim\":\n",
        "        fim_entrevista = True\n",
        "        finalizar_entrevista()\n",
        "        return\n",
        "\n",
        "    if not system_instruction:\n",
        "        with feedback_output:\n",
        "            feedback_output.clear_output()\n",
        "            format_markdown(\"**Por favor, configure a entrevista primeiro.**\")\n",
        "        return\n",
        "\n",
        "    prompt_com_resposta = f\"{system_instruction}\\n\\nMinha resposta foi: {resposta_usuario}\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt_com_resposta)\n",
        "        feedback = response.text\n",
        "\n",
        "        respostas_usuario.append(resposta_usuario)\n",
        "        historico_conversa.append(f\"**Você:** {resposta_usuario}\")\n",
        "        historico_conversa.append(f\"**Feedback:** {feedback}\")\n",
        "        atualizar_historico()\n",
        "\n",
        "        with feedback_output:\n",
        "            feedback_output.clear_output()\n",
        "            format_markdown(f\"**Feedback:**\\n{feedback}\")\n",
        "\n",
        "        if not fim_entrevista:\n",
        "            gerar_proxima_pergunta()\n",
        "\n",
        "    except Exception as e:\n",
        "        with feedback_output:\n",
        "            feedback_output.clear_output()\n",
        "            format_markdown(f\"**Erro ao gerar feedback:** {e}\")\n",
        "\n",
        "def gerar_proxima_pergunta():\n",
        "    global system_instruction, fim_entrevista, perguntas_realizadas\n",
        "\n",
        "    if fim_entrevista:\n",
        "        with pergunta_output:\n",
        "            pergunta_output.clear_output()\n",
        "            format_markdown(\"**A entrevista foi encerrada.**\")\n",
        "        return\n",
        "\n",
        "    if not system_instruction:\n",
        "        with pergunta_output:\n",
        "            pergunta_output.clear_output()\n",
        "            format_markdown(\"**Por favor, configure a entrevista primeiro.**\")\n",
        "        return\n",
        "\n",
        "    prompt_pergunta = f\"{system_instruction}\\n\\nFaça a próxima pergunta.\"\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt_pergunta)\n",
        "        pergunta = response.text\n",
        "\n",
        "        perguntas_realizadas.append(pergunta)\n",
        "        historico_conversa.append(f\"**Entrevistador:** {pergunta}\")\n",
        "        atualizar_historico()\n",
        "\n",
        "        with pergunta_output:\n",
        "            pergunta_output.clear_output()\n",
        "            format_markdown(f\"**Entrevistador:**\\n{pergunta}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        with pergunta_output:\n",
        "            pergunta_output.clear_output()\n",
        "            format_markdown(f\"**Erro ao gerar pergunta:** {e}\")\n",
        "\n",
        "def atualizar_historico():\n",
        "    with historico_output:\n",
        "        historico_output.clear_output()\n",
        "        for item in historico_conversa:\n",
        "            format_markdown(f\"{item}\\n\\n\")\n",
        "\n",
        "proxima_pergunta_button = widgets.Button(description=\"Próxima Pergunta\")\n",
        "proxima_pergunta_button.on_click(lambda b: gerar_proxima_pergunta())\n",
        "enviar_config_button.on_click(configurar_entrevista)\n",
        "enviar_resposta_button.on_click(enviar_resposta)\n",
        "\n",
        "# Exibe os widgets\n",
        "display(vaga_input)\n",
        "display(area_input)\n",
        "display(tom_input)\n",
        "display(conhecimentos_input)\n",
        "display(enviar_config_button)\n",
        "display(pergunta_output)\n",
        "display(resposta_input)\n",
        "display(enviar_resposta_button)\n",
        "display(feedback_output)\n",
        "display(widgets.Label(value=\"Histórico da Conversa:\"))\n",
        "display(historico_output)\n",
        "display(proxima_pergunta_button)"
      ]
    }
  ]
}