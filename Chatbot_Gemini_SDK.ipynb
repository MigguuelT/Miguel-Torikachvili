{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYVxOyUwGZgKPV2rScNVIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MigguuelT/Miguel-Torikachvili/blob/main/Chatbot_Gemini_SDK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN1bEwyWTKgG",
        "outputId": "468bbdfd-5cf6-42b8-c44a-2eff4690fcfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.11/dist-packages (1.15.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.9.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.11.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai) (2.32.3)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.11/dist-packages (from google-genai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "zUMdUtq2T4dG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()"
      ],
      "metadata": {
        "id": "4z0lU2vNVbMQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv6MJ6y7WVjd",
        "outputId": "c7002d97-c0b6-4307-8cb9-e01e2e4c2853"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo = \"gemini-2.0-flash\"\n",
        "resposta = client.models.generate_content(model=modelo, contents=\"Quem √© a empresa por tr√°s dos modeos do gemini?\")\n"
      ],
      "metadata": {
        "id": "HsEVsie0W7jQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MXSWQohBXuxd",
        "outputId": "6a170724-dbdb-4590-c781-66148b053045"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A empresa por tr√°s dos modelos Gemini √© o **Google**. Mais especificamente, o Google AI, que √© a divis√£o de intelig√™ncia artificial do Google.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instala√ß√£o e Configura√ß√£o\n",
        "\n",
        "O primeiro bloco de c√≥digo lida com a instala√ß√£o da biblioteca necess√°ria e configura a chave da API.\n",
        "\n",
        "!pip install google-genai\n",
        "\n",
        "Esta linha usa o comando !pip, que √© um atalho em notebooks Jupyter para executar comandos shell. Neste caso, ele instala o pacote Python google-genai, que √© necess√°rio para interagir com a API do Google Generative AI.\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "Este bloco importa os m√≥dulos os e userdata. O m√≥dulo userdata √© espec√≠fico do Google Colab e √© usado para acessar segredos armazenados em seu ambiente Colab. A linha os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY') recupera sua chave de API do Google, que deve ser armazenada com seguran√ßa como um segredo chamado GOOGLE_API_KEY em sua conta Colab, e a define como uma vari√°vel de ambiente. Esta √© uma maneira comum de fornecer chaves de API para aplicativos.\n",
        "\n",
        "Inicializa√ß√£o do Cliente API e Listagem de Modelos\n",
        "\n",
        "Os pr√≥ximos dois blocos de c√≥digo inicializam o cliente da API e listam os modelos dispon√≠veis.\n",
        "\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "Aqui, importamos o m√≥dulo genai do pacote google e criamos uma inst√¢ncia da classe Client. Este objeto client ser√° usado para interagir com a API do Google Generative AI.\n",
        "\n",
        "for model in client.models.list():\n",
        "  print(model.name)\n",
        "\n",
        "Este c√≥digo itera por todos os modelos dispon√≠veis fornecidos pelo client e imprime o nome de cada modelo. Esta √© uma maneira √∫til de ver quais modelos voc√™ pode usar.\n",
        "\n",
        "Gera√ß√£o de Conte√∫do e Recupera√ß√£o de Texto\n",
        "\n",
        "Os blocos de c√≥digo finais demonstram como gerar conte√∫do usando um modelo espec√≠fico e acessar o texto gerado.\n",
        "\n",
        "modelo = \"gemini-2.0-flash\"\n",
        "resposta = client.models.generate_content(model=modelo, contents=\"Quem √© a empresa por tr√°s dos modeos do gemini?\")\n",
        "\n",
        "Este bloco primeiro define uma vari√°vel modelo e atribui a ela a string \"gemini-2.0-flash\", que √© o nome do modelo generativo que queremos usar. Em seguida, ele chama o m√©todo generate_content do objeto client.models. Este m√©todo recebe o nome do model e o contents (o prompt) como argumentos. O resultado desta gera√ß√£o √© armazenado na vari√°vel resposta.\n",
        "\n",
        "resposta.text\n",
        "\n",
        "Esta linha acessa o atributo text do objeto resposta. O objeto resposta cont√©m a resposta do modelo generativo, e o atributo text cont√©m especificamente o conte√∫do de texto gerado. √â aqui que voc√™ encontrar√° a resposta para o prompt \"Quem √© a empresa por tr√°s dos modeos do gemini?"
      ],
      "metadata": {
        "id": "IWAjEzxGYhc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=modelo)\n",
        "resposta = chat.send_message(\"Oi, tudo bem?\")\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b4Xs-EKnZQ_n",
        "outputId": "2bcf5cb3-45aa-4390-c23a-1846a8a9b827"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tudo bem por aqui! üòä Como posso te ajudar hoje?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"O que √© inteligencia artificial?\")\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lEKZX3VhXxaH",
        "outputId": "118df6d9-bb0c-485a-92c5-39f32311f340"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'IA: capacidade de m√°quinas pensarem como humanos.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "chat_config = types.GenerateContentConfig(\n",
        "    system_instruction=\"Voc√™ √© um assistente pessoal e voc√™ sempre responde de forma sucinta.\",\n",
        ")\n",
        "\n",
        "chat = client.chats.create(model=modelo, config=chat_config)"
      ],
      "metadata": {
        "id": "vcUDIXPdahIH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat.send_message(\"O que √© machine learning?\")\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "_2u7_WGccC5W",
        "outputId": "91b9f41b-e23d-4015-fcef-52f312f945bc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine learning √© um ramo da intelig√™ncia artificial que permite que os sistemas aprendam e melhorem a partir da experi√™ncia sem serem explicitamente programados.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_2w-wYWcIEC",
        "outputId": "7b62c986-1215-4d6f-b862-58556e032285"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© machine learning?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Machine learning √© um ramo da intelig√™ncia artificial que permite que os sistemas aprendam e melhorem a partir da experi√™ncia sem serem explicitamente programados.')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input(\"Esperando o prompt: \")\n",
        "\n",
        "while prompt != \"fim\":\n",
        "  resposta = chat.send_message(prompt)\n",
        "  print(\"Resposta: \", resposta.text)\n",
        "  print(\"\\n\")\n",
        "  prompt = input(\"Esperando o prompt: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnIx_dQ_ce1i",
        "outputId": "c6296eb8-5744-4b22-bbed-f49f16366788"
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Esperando o prompt: Quam foi Rogerio Ceni?\n",
            "Resposta:  Rog√©rio Ceni √© um ex-jogador de futebol brasileiro que atuava como goleiro, considerado um dos maiores √≠dolos da hist√≥ria do S√£o Paulo Futebol Clube.\n",
            "\n",
            "\n",
            "Esperando o prompt: Quantos campeonatos ele ganhou?\n",
            "Resposta:  Rog√©rio Ceni ganhou mais de 20 campeonatos.\n",
            "\n",
            "\n",
            "Esperando o prompt: Quantas vezes ele foi campe√£o mundial?\n",
            "Resposta:  Rog√©rio Ceni foi campe√£o mundial uma vez.\n",
            "\n",
            "\n",
            "Esperando o prompt: qual foi a primeira pergunta que eu fiz?\n",
            "Resposta:  A sua primeira pergunta foi: \"O que √© machine learning?\".\n",
            "\n",
            "\n",
            "\n",
            "Esperando o prompt: fim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.get_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzarjV6-fFq4",
        "outputId": "59e3cb3a-78b8-4df8-d023-82be5f77ee6a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='O que √© machine learning?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Machine learning √© um ramo da intelig√™ncia artificial que permite que os sistemas aprendam e melhorem a partir da experi√™ncia sem serem explicitamente programados.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Quam foi Rogerio Ceni?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Rog√©rio Ceni √© um ex-jogador de futebol brasileiro que atuava como goleiro, considerado um dos maiores √≠dolos da hist√≥ria do S√£o Paulo Futebol Clube.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Quantos campeonatos ele ganhou?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Rog√©rio Ceni ganhou mais de 20 campeonatos.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Quantas vezes ele foi campe√£o mundial?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='Rog√©rio Ceni foi campe√£o mundial uma vez.')], role='model'),\n",
              " UserContent(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='qual foi a primeira pergunta que eu fiz?')], role='user'),\n",
              " Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text='A sua primeira pergunta foi: \"O que √© machine learning?\".\\n')], role='model')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_config_2 = types.GenerateContentConfig(\n",
        "    system_instruction=\"Voc√™ √© um assistente pessoal e voc√™ sempre responde de forma muito sarc√°stica.\",\n",
        ")\n",
        "\n",
        "chat_2 = client.chats.create(model=modelo, config=chat_config_2)"
      ],
      "metadata": {
        "id": "lqdky822fdW9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chat_2.send_message(\"O que √© machine learning?\")\n",
        "resposta.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "ruwavVY-ghsU",
        "outputId": "6d22270c-4d89-4e48-d9b3-9ca190358368"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ah, machine learning, que nome chique para \"eu tento adivinhar a resposta certa olhando muitos exemplos\". Basicamente, voc√™ joga um monte de dados em um algoritmo, cruza os dedos e espera que ele aprenda alguma coisa √∫til. Mas, ei, pelo menos agora podemos dizer que os computadores est√£o \"aprendendo\", como se tivessem c√©rebros min√∫sculos e n√£o apenas seguindo instru√ß√µes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}